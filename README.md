# AMR-ViT: Adaptive Multi-Register Vision Transformer

A novel Vision Transformer architecture featuring an adaptive register mechanism that dynamically adjusts based on input complexity, improving both efficiency and interpretability across medical and natural image datasets.

## ğŸ¯ Key Features

- **Adaptive Register Bank**: Dynamic gating mechanism that selectively activates 0-8 register tokens based on image content
- **Cross-Domain Performance**: Tested on both medical imaging (ChestMNIST) and natural images (CIFAR-10)
- **Interpretability**: Built-in attention visualization for understanding model focus
- **Efficient Training**: Mixed precision training with AMP for faster convergence

## ğŸ“Š Results

| Metric | Standard ViT (Baseline) | AMR-ViT (Ours) | Improvement |
|--------|------------------------|----------------|-------------|
| CIFAR-10 Test Accuracy | Baseline | Enhanced | +X.XX% |
| Model Type | Static | Adaptive | - |
| Registers Used | 0 (Fixed) | 0-8 (Dynamic) | Flexible |

## ğŸ—ï¸ Architecture

### Core Components

1. **FlexiblePatchEmbed**: Handles variable input resolutions (28Ã—28 for medical, 32Ã—32 for natural images)
2. **Adaptive Controller**: Neural network that generates gate weights for register selection
3. **Gated Register Bank**: Learnable tokens that are dynamically weighted per input
4. **InterpretableBlock**: Custom transformer layers with attention weight extraction

### Model Architecture

```
Input Image â†’ Patch Embedding â†’ [CLS Token + Adaptive Registers + Patches]
    â†“
Positional Encoding
    â†“
Transformer Blocks (with Attention Visualization)
    â†“
Layer Norm â†’ Classification Head
```

## ğŸ“¦ Installation

```bash
# Clone the repository
git clone https://github.com/yourusername/amr-vit-project.git
cd amr-vit-project

# Install dependencies
pip install -r requirements.txt
```

## ğŸš€ Quick Start

### Training on MedMNIST (ChestMNIST)

```python
from amr_vit import AdaptiveRegisterViT
import torch

# Initialize model for medical imaging
model = AdaptiveRegisterViT(
    img_size=28,
    patch_size=4,
    in_chans=1,        # Grayscale
    num_classes=14,    # 14 diseases
    embed_dim=192,
    depth=12,
    num_heads=3,
    max_registers=8
)

# Training code available in AMR_ViT.ipynb
```

### Training on CIFAR-10

```python
# Initialize model for natural images
model = AdaptiveRegisterViT(
    img_size=32,
    patch_size=4,
    in_chans=3,        # RGB
    num_classes=10,
    embed_dim=128,
    depth=6,
    num_heads=4,
    max_registers=8
)
```

## ğŸ““ Notebooks

- **`AMR_ViT.ipynb`**: Complete implementation with training, evaluation, and visualization
  - MedMNIST training and evaluation
  - CIFAR-10 experiments
  - Attention map visualization
  - Comparative analysis with baseline ViT

## ğŸ”¬ Experiments

### Dataset Support

1. **ChestMNIST** (MedMNIST)
   - 14-class multi-label classification
   - Grayscale chest X-rays (28Ã—28)
   - Focus on artifact suppression and disease detection

2. **CIFAR-10**
   - 10-class single-label classification
   - RGB natural images (32Ã—32)
   - Testing generalization across domains

### Visualization

The model provides interpretable attention maps showing:
- Which regions the model focuses on
- How register gates adapt to different inputs
- Comparison between healthy and pathological cases

## ğŸ› ï¸ Requirements

- Python 3.8+
- PyTorch 1.12+
- torchvision
- medmnist
- numpy
- pandas
- matplotlib

See `requirements.txt` for complete dependencies.

## ğŸ“– Usage Examples

### Attention Visualization

```python
from visualization import visualize_prediction

# Visualize attention on a specific sample
visualize_prediction(model, val_dataset, index=10)
```

### Model Evaluation

```python
from utils import evaluate_model

# Evaluate on test set
test_accuracy = evaluate_model(model, test_loader)
print(f"Test Accuracy: {test_accuracy:.2f}%")
```

## ğŸ† Team Members

- **Sai Prasanna Kamkolam**
- **Vishnu Vardhan Reddy Pappula**
- **Vineeth Kumar Mamidipally**

## ğŸ“„ Project Structure

```
amr-vit-project/
â”œâ”€â”€ AMR_ViT.ipynb          # Main notebook with all experiments
â”œâ”€â”€ README.md              # This file
â”œâ”€â”€ requirements.txt       # Python dependencies
â”œâ”€â”€ LICENSE               # MIT License
â””â”€â”€ .gitignore           # Git ignore rules
```

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

## ğŸ“ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- MedMNIST dataset creators
- PyTorch and torchvision teams
- Vision Transformer (ViT) paper authors

## ğŸ“§ Contact

For questions or collaborations, please open an issue or contact the team members.

---

â­ If you find this project useful, please consider giving it a star!

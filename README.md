# AMR-ViT: Adaptive Multi-Register Vision Transformer

AMR-ViT is an adaptive Vision Transformer architecture that introduces **dynamically gated register tokens** to adjust representational capacity based on input complexity, improving both **performance** and **interpretability** across medical and natural image domains.

---

## ğŸ“Œ Overview

Conventional Vision Transformers rely on a **fixed token structure**, which limits their ability to adapt to inputs of varying complexity.  
This limitation is particularly critical in **medical imaging**, where subtle visual patterns demand richer contextual reasoning.

**AMR-ViT (Adaptive Multi-Register Vision Transformer)** addresses this challenge by introducing:

- A bank of **learnable register tokens**
- A **gating controller** that dynamically activates registers for each input image

This enables **input-aware computation**, improving accuracy and interpretability while maintaining computational efficiency.

---

## âœ¨ Key Features

### ğŸ”€ Adaptive Register Tokens (0â€“8 Registers)
- Registers are dynamically activated based on image complexity.

### ğŸ›ï¸ Gating Controller
- Learns to predict register activation from global image context.

### ğŸ“ˆ Performance Improvement
- Achieves a **+3.78% accuracy gain** over a standard ViT baseline on **CIFAR-10**.

### ğŸ§  Interpretability
- Attention maps and register activations provide insight into model decision-making.

### âš™ï¸ Lightweight Design
- Minimal increase in parameters and FLOPs compared to baseline ViT.

---

## ğŸ§  Architecture

### AMR-ViT Processing Pipeline

Input Image
â†“
Patch Embedding
â†“
Global Context Extraction
â†“
Gating Controller
â†“
Adaptive Register Activation
â†“
Transformer Encoder Stack
â†“
Classification Head


### Conceptual Flow

1. Input images are converted into patch embeddings.
2. A global descriptor summarizes the image content.
3. A gating network predicts activation weights for register tokens.
4. Activated registers are injected into the token sequence.
5. Transformer encoders refine representations for final classification.

---

## ğŸ“Š Experimental Results

| Model            | Test Accuracy | Register Usage |
|------------------|---------------|----------------|
| Standard ViT     | 63.97%        | Fixed (0)     |
| **AMR-ViT (Ours)** | **67.75%**    | 0â€“8 (Dynamic) |

---

## ğŸ” Interpretability Insights

- Simple images activate **fewer registers (1â€“2)**
- Complex images activate **more registers (6â€“8)**
- Attention maps demonstrate stronger focus on task-relevant regions
- For **ChestMNIST**, attention aligns with clinically meaningful lung regions

---

## ğŸ“ Repository Structure
```bibtex
Adaptive-ViT-Registers/
â”‚
â”œâ”€â”€ AMR_ViT.ipynb        # Training, evaluation, and visualization
â”œâ”€â”€ SETUP_GUIDE.md      # Environment setup instructions
â”œâ”€â”€ requirements.txt    # Python dependencies
â”œâ”€â”€ AMR-ViT_Report.pdf  # Project report / research paper
â”œâ”€â”€ LICENSE             # License information
â””â”€â”€ README.md           # Project documentation
```
---

## ğŸ§° Installation

```bibtex
git clone https://github.com/vineethk297/Adaptive-ViT-Registers.git
cd Adaptive-ViT-Registers
pip install -r requirements.txt
```

---

## ğŸ“œ Citation

```bibtex
@article{amrvit2024,
  title={Adaptive Multi-Register Vision Transformer for Medical and Natural Image Classification},
  author={Kamkolam, Sai Prasanna and Pappula, Vishnu Vardhan Reddy and Mamidipally, Vineeth Kumar},
  year={2024},
  institution={Florida Atlantic University}
}

---
```
## ğŸ“„ License

This project is licensed under the MIT License.  
See the LICENSE file for more details.

